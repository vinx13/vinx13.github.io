<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Wuwei Lin</title>
    <link>http://wuwei.io/post/</link>
    <description>Recent content in Posts on Wuwei Lin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Jun 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://wuwei.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Some thoughts on linalg</title>
      <link>http://wuwei.io/post/2018/06/some-thoughts-on-linalg/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://wuwei.io/post/2018/06/some-thoughts-on-linalg/</guid>
      <description>So in the next two weeks I&amp;rsquo;m going to work on refactoring linalg of Shogun. The data types in Shogun, SGVector&amp;lt;T&amp;gt;, SGMatrix&amp;lt;T&amp;gt; are templated classes which maintain a piece of memory in either CPU or GPU and provide basic operations like array-like element access. linalg::, is the upper level module working with SGVector, SGMatrix, providing linalg algebra operations, like matrix additions / multiplications.
Though SGVector and SGMatrix, as template classes, are capable to represent different data types (float32_t, float64_t, etc.</description>
    </item>
    
    <item>
      <title>New Transformers in Shogun</title>
      <link>http://wuwei.io/post/2018/05/new-transformers-in-shogun/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://wuwei.io/post/2018/05/new-transformers-in-shogun/</guid>
      <description>The first two weeks of GSoC have ended. Here I will share my progress so far.
In the first two weeks, I have been mostly working on the new transformer class, CTransformer. It is introduced mainly for two purposes: 1) Providing generic and unified interface for data transformation. 2) Make features immutable.
In Shogun, we have preprocessors and converters, both operating on features and applying some transformations. The major distinction between them I think is that preprocessors support on-the-fly evaluation: we can add several preprocessors by add_preprocessor and then apply them when calling get_feature_vector.</description>
    </item>
    
    <item>
      <title>GSoC&#39;18: Overview</title>
      <link>http://wuwei.io/post/2018/05/gsoc18-overview/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://wuwei.io/post/2018/05/gsoc18-overview/</guid>
      <description>Projects of GSoC&amp;rsquo;18 were announced last week and I am happy that I will be working on Shogun over this summer. In this post, I am pleased to share some details of my project.
Shogun is a versatile machine learning toolbox that offers a bunch of convinent and unified algorithm implementations. Written in C++ and providing interface in different languages via SWIG, it is a great place to hack.</description>
    </item>
    
  </channel>
</rss>